{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage import transform\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_characters = utils.load_characters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in map_characters.items():\n",
    "    print(\"{} -> {}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pics, labels = utils.load_pictures(map_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Images shape:\")\n",
    "print(pics.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.show_random(pics, labels, map_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train / Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pics_train, labels_train, pics_val, labels_val = utils.split(pics, labels, p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data:\")\n",
    "print(\"X: {}\".format(pics_train.shape))\n",
    "print(\"Y: {}\".format(labels_train.shape))\n",
    "print()\n",
    "print(\"Validation data:\")\n",
    "print(\"X: {}\".format(pics_val.shape))\n",
    "print(\"Y: {}\".format(labels_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W, C = pics[0].shape\n",
    "NUM_CLASSES = len(map_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_architecture():\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.uint8, shape=[None, H, W, 3], name=\"x\")\n",
    "    y = tf.placeholder(tf.uint8, shape=[None, NUM_CLASSES], name=\"y\")\n",
    "    \n",
    "    dropout_rate = tf.placeholder_with_default(0.3, shape=(), name=\"dropout_rate\")\n",
    "    \n",
    "    is_training = tf.placeholder_with_default(False, shape=(), name='is_training')\n",
    "    \n",
    "    init = tf.contrib.layers.xavier_initializer()\n",
    "    \n",
    "    out = tf.divide(x, 255)\n",
    "    \n",
    "    out = tf.layers.conv2d(out, filters=16, kernel_size=[3,3], activation=tf.nn.elu, kernel_initializer=init, padding=\"same\")\n",
    "    out = tf.layers.max_pooling2d(out, pool_size=(2, 2), strides=[2,2])\n",
    "    \n",
    "    out = tf.layers.dropout(out, rate=dropout_rate, training=is_training)\n",
    "    \n",
    "    out = tf.layers.conv2d(out, filters=32, kernel_size=[3,3], activation=tf.nn.elu, kernel_initializer=init, padding=\"same\")\n",
    "    out = tf.layers.max_pooling2d(out, pool_size=(2, 2), strides=[2,2])\n",
    "    \n",
    "    out = tf.layers.dropout(out, rate=dropout_rate, training=is_training)\n",
    "    \n",
    "    out = tf.layers.conv2d(out, filters=64, kernel_size=[3,3], activation=tf.nn.elu, kernel_initializer=init, padding=\"same\")\n",
    "    out = tf.layers.max_pooling2d(out, pool_size=(2, 2), strides=[2,2])\n",
    "    \n",
    "    out = tf.layers.dropout(out, rate=dropout_rate, training=is_training)\n",
    "    \n",
    "    out = tf.contrib.layers.flatten(out)\n",
    "    \n",
    "    out = tf.layers.dense(out, units=512, activation=tf.nn.relu, kernel_initializer=init)\n",
    "    \n",
    "    out = tf.layers.dropout(out, rate=dropout_rate, training=is_training)\n",
    "    \n",
    "    out = tf.layers.dense(out, units=NUM_CLASSES, kernel_initializer=init)\n",
    "    \n",
    "    return x, y, is_training, dropout_rate, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_loss(y, out):\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=out, name=\"mean_loss\")\n",
    "    loss = tf.reduce_mean(loss, name=\"loss\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_accuracy(y, out):\n",
    "    pred = tf.argmax(out, axis=-1)\n",
    "    gt = tf.argmax(y, axis=-1)\n",
    "    \n",
    "    matches = tf.equal(pred, gt)\n",
    "    \n",
    "    return tf.reduce_mean(tf.cast(matches, tf.float32), name=\"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer(loss):\n",
    "    opt = tf.train.AdamOptimizer()\n",
    "    return opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_scalars(m):\n",
    "    for k, v in m.items():\n",
    "        tf.summary.scalar(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_images(m):\n",
    "    for k, v in m.items():\n",
    "        tf.summary.image(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainable_parameters():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        # shape is an array of tf.Dimension\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    x, y, is_training, dropout_rate, out = load_architecture()\n",
    "    loss = load_loss(y, out)\n",
    "    acc = load_accuracy(y, out)\n",
    "    upd = load_trainer(loss)\n",
    "    \n",
    "    register_scalars({\"info_loss\": loss, \"info_acc\": acc})\n",
    "    register_images({\"input\": x})\n",
    "\n",
    "    info = tf.summary.merge_all()\n",
    "    \n",
    "    return x, y, is_training, dropout_rate, out, loss, acc, upd, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session():\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, model, pics_train, labels_train, pics_val, labels_val, epochs, batch_size, train_writer, val_writer, use_dropout=False):\n",
    "    N, _, _, _ = pics_train.shape\n",
    "    idxs = np.arange(N)\n",
    "    \n",
    "    x, y, is_training, dropout_rate, out, loss, acc, upd, info = model\n",
    "    \n",
    "    d_rate = 0.3 if use_dropout else 0.\n",
    "    \n",
    "    i=0\n",
    "\n",
    "    for ep in tqdm(range(epochs)):\n",
    "        np.random.shuffle(idxs)\n",
    "        pics_train = pics_train[idxs]\n",
    "        labels_train = labels_train[idxs]\n",
    "\n",
    "        for b in range(0, N, batch_size):\n",
    "            X_batch = pics_train[b:b+batch_size]\n",
    "            Y_batch = labels_train[b:b+batch_size]\n",
    "\n",
    "            if X_batch.shape[0] < BATCH_SIZE:\n",
    "                break\n",
    "\n",
    "            graph_info, _ = sess.run([info, upd], feed_dict={x: X_batch, y: Y_batch, is_training: True, dropout_rate: d_rate})\n",
    "            train_writer.add_summary(graph_info, i)\n",
    "            \n",
    "            graph_info, = sess.run([info], feed_dict={x: pics_val, y: labels_val, is_training: False, dropout_rate: d_rate})\n",
    "            val_writer.add_summary(graph_info, i)\n",
    "            \n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    img_batch = np.reshape(img, [1, H, W, 3])\n",
    "    graph_out, = sess.run([out], feed_dict={x: img_batch})\n",
    "    char = np.argmax(np.squeeze(graph_out))\n",
    "    plt.imshow(img)\n",
    "    plt.title(map_characters[char])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pics_train_sm, labels_train_sm = utils.get_small_dataset(pics_train, labels_train, p=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New training dataset size: {}\".format(pics_train_sm.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "print(\"Trainable parameters: {}\".format(trainable_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LOGS_DIR = \"logs\"\n",
    "\n",
    "sess = load_session()\n",
    "\n",
    "t_writer = tf.summary.FileWriter(os.path.join(LOGS_DIR, \"overfit\", \"train\"), graph=sess.graph)\n",
    "v_writer = tf.summary.FileWriter(os.path.join(LOGS_DIR, \"overfit\", \"val\"), graph=sess.graph)\n",
    "\n",
    "train(sess, model, pics_train_sm, labels_train_sm, pics_val, labels_val, EPOCHS, BATCH_SIZE, t_writer, v_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(pics_train.shape[0])\n",
    "predict(pics_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(pics_val.shape[0])\n",
    "predict(pics_val[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop-env",
   "language": "python",
   "name": "workshop-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
