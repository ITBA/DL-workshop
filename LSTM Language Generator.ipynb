{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_PARAGRAPH_LEN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(_dir):\n",
    "    ret = []\n",
    "    for each in os.listdir(_dir):\n",
    "        full_path = os.path.join(_dir, each)\n",
    "        if each.endswith(\"txt\"):\n",
    "            with open(full_path, \"rb\") as f:\n",
    "                aux = f.read().decode(\"utf-8\").split('\\n\\n')\n",
    "                for paragraph in aux:\n",
    "                    paragraph = paragraph.strip('\\n')\n",
    "                    paragraph += '\\n'\n",
    "                    if len(paragraph) < MIN_PARAGRAPH_LEN:\n",
    "                        continue\n",
    "                    ret.append(paragraph)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = load_data(\"./speeches/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Paragraphs: {}\".format(len(ps)))\n",
    "\n",
    "arr = np.asarray([len(x) for x in ps])\n",
    "\n",
    "print(\"Mean {}\".format(np.mean(arr)))\n",
    "print(\"Median {}\".format(np.median(arr)))\n",
    "print(\"Std {}\".format(np.std(arr)))\n",
    "print(\"Max {}\".format(np.max(arr)))\n",
    "print(\"Min {}\".format(np.min(arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    idx = random.choice(range(len(ps)))\n",
    "    print(ps[idx])\n",
    "    print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraphs):\n",
    "    chars = set()\n",
    "    \n",
    "    for each in paragraphs:\n",
    "        chars.update(set(each))\n",
    "    \n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "    \n",
    "    # Add START token\n",
    "    aux = len(char_to_ix)\n",
    "    char_to_ix[\"<START>\"] = aux\n",
    "    ix_to_char[aux] = \"<START>\"\n",
    "    \n",
    "    vocab_size = len(char_to_ix)\n",
    "\n",
    "    max_p = max([len(i) for i in paragraphs]) + 1 # Plus one because of the START token\n",
    "    \n",
    "    ret = np.zeros(shape=(len(paragraphs), max_p, vocab_size))\n",
    "    lens = np.zeros(shape=len(paragraphs))\n",
    "\n",
    "    for idx, each in enumerate(paragraphs):\n",
    "        lens[idx] = len(each) + 1\n",
    "        for i in range(max_p - len(each) - 1):\n",
    "            each += '\\n'\n",
    "\n",
    "        aux = np.zeros(shape=(max_p, vocab_size))\n",
    "        aux[0][char_to_ix[\"<START>\"]] = 1\n",
    "        for i, c in enumerate(each):\n",
    "            aux[i+1][char_to_ix[c]] = 1\n",
    "        ret[idx] = aux\n",
    "        \n",
    "    return ret, lens, char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lens, char_to_ix, ix_to_char = preprocess(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "INPUT_SIZE = len(ix_to_char)\n",
    "\n",
    "TIMES = 32\n",
    "N_HIDDEN = 512\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "x = tf.placeholder(tf.float32, shape=(None, TIMES, INPUT_SIZE), name=\"x\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, TIMES, INPUT_SIZE))\n",
    "seq_len = tf.placeholder(tf.int64, shape=(None), name=\"seq_len\")\n",
    "\n",
    "x_2 = tf.unstack(x, axis=1)\n",
    "\n",
    "init_state_c_1 = tf.placeholder(tf.float32, shape=[None, N_HIDDEN], name=\"init_state_c_1\")\n",
    "init_state_h_1 = tf.placeholder(tf.float32, shape=[None, N_HIDDEN], name=\"init_state_h_1\")\n",
    "\n",
    "init_state_c_2 = tf.placeholder(tf.float32, shape=[None, N_HIDDEN], name=\"init_state_c_2\")\n",
    "init_state_h_2 = tf.placeholder(tf.float32, shape=[None, N_HIDDEN], name=\"init_state_h_2\")\n",
    "\n",
    "cell_1 = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN)\n",
    "cell_2 = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN)\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell_1, cell_2])\n",
    "    \n",
    "t_1 = tf.contrib.rnn.LSTMStateTuple(init_state_c_1, init_state_h_1)\n",
    "t_2 = tf.contrib.rnn.LSTMStateTuple(init_state_c_2, init_state_h_2)\n",
    "\n",
    "outputs, states = tf.contrib.rnn.static_rnn(cell, x_2, dtype=tf.float32, sequence_length=seq_len, initial_state=(t_1, t_2))\n",
    "\n",
    "states_0 = tf.nn.rnn_cell.LSTMStateTuple(tf.identity(states[0][0], name=\"states_0_c\"), tf.identity(states[0][1], name=\"states_0_h\"))\n",
    "states_1 = tf.nn.rnn_cell.LSTMStateTuple(tf.identity(states[1][0], name=\"states_1_c\"), tf.identity(states[1][1], name=\"states_1_h\"))\n",
    "\n",
    "states = (states_0, states_1)\n",
    "\n",
    "outputs_2 = tf.stack(outputs, axis=1)\n",
    "\n",
    "out = tf.layers.dense(outputs_2, units=INPUT_SIZE, kernel_initializer=init, name=\"out\")\n",
    "\n",
    "out_softmax = tf.nn.softmax(out, name=\"out_softmax\")\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=y))\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "merge = tf.summary.merge_all()\n",
    "\n",
    "upd = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(max_=1000, T=None):\n",
    "\n",
    "    pred = \"<START>\"\n",
    "    \n",
    "    c_1 = np.zeros((1, N_HIDDEN))\n",
    "    h_1 = np.zeros((1, N_HIDDEN))\n",
    "    \n",
    "    c_2 = np.zeros((1, N_HIDDEN))\n",
    "    h_2 = np.zeros((1, N_HIDDEN))\n",
    "    \n",
    "    ret = []\n",
    "        \n",
    "    while True:\n",
    "        \n",
    "        in_ = np.zeros(shape=(1, TIMES, INPUT_SIZE), dtype=np.uint)\n",
    "        in_[0, 0, char_to_ix[pred]] = 1\n",
    "\n",
    "        if T is None:\n",
    "            net_out, net_states = sess.run([out_softmax, states], feed_dict={x: in_, init_state_c_1: c_1, init_state_h_1: h_1, init_state_c_2: c_2, init_state_h_2: h_2, seq_len: np.ones(shape=(1,))})\n",
    "            c_1, h_1 = net_states[0].c, net_states[0].h\n",
    "            c_2, h_2 = net_states[1].c, net_states[1].h\n",
    "            p = np.squeeze(net_out)[0]\n",
    "        else:\n",
    "            net_out, net_states = sess.run([out, states], feed_dict={x: in_, init_state_c_1: c_1, init_state_h_1: h_1, init_state_c_2: c_2, init_state_h_2: h_2, seq_len: np.ones(shape=(1,))})\n",
    "            c_1, h_1 = net_states[0].c, net_states[0].h\n",
    "            c_2, h_2 = net_states[1].c, net_states[1].h\n",
    "            p = np.squeeze(net_out)[0]\n",
    "            p = np.exp(p/T) / np.sum(np.exp(p/T))\n",
    "            \n",
    "        char_out = ix_to_char[int(np.random.choice(np.arange(INPUT_SIZE), p=p))]\n",
    "        ret.append(char_out)\n",
    "\n",
    "        pred = char_out\n",
    "                                                                         \n",
    "        if char_out == '\\n' or len(ret) > max_:\n",
    "            break\n",
    "        \n",
    "    return ret                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "N, M, V = data.shape\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "zeros = np.zeros(shape=(BATCH_SIZE))\n",
    "times_minus_one = (TIMES - 1) * np.ones(shape=(BATCH_SIZE))\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logs/train', sess.graph)\n",
    "\n",
    "counter = 0\n",
    "for e in tqdm(range(EPOCHS)):\n",
    "    \n",
    "    idxs = np.random.choice(N, BATCH_SIZE, replace=False)\n",
    "    batch = data[idxs]\n",
    "    batch_lens = lens[idxs].astype(np.int32)\n",
    "    \n",
    "    ts = (M-1) // TIMES # + 1\n",
    "    \n",
    "    # Initial state\n",
    "    c_1 = np.zeros((BATCH_SIZE, N_HIDDEN))\n",
    "    h_1 = np.zeros((BATCH_SIZE, N_HIDDEN))\n",
    "\n",
    "    c_2 = np.zeros((BATCH_SIZE, N_HIDDEN))\n",
    "    h_2 = np.zeros((BATCH_SIZE, N_HIDDEN))\n",
    "    \n",
    "    if e % 10 == 0:\n",
    "        print(\"\".join(test(max_=100)))\n",
    "    \n",
    "    for t in range(ts):\n",
    "        batch_x = batch[:, t*TIMES:TIMES*(t+1), :]\n",
    "        batch_y = batch[:, t*TIMES+1:TIMES*(t+1)+1, :]\n",
    "        \n",
    "        batch_lens_aux = batch_lens -  (TIMES * t)\n",
    "        \n",
    "        batch_lens_aux = np.maximum(zeros, batch_lens_aux)\n",
    "        batch_lens_aux = np.minimum(times_minus_one, batch_lens_aux)\n",
    "        \n",
    "        batch_lens_aux = batch_lens_aux.astype(np.uint8)\n",
    "        \n",
    "        non_zero_idxs = batch_lens_aux > 0\n",
    "        batch_lens_aux = batch_lens_aux[non_zero_idxs]\n",
    "\n",
    "        batch_x = batch_x[non_zero_idxs, :, :]\n",
    "        batch_y = batch_y[non_zero_idxs, :, :]\n",
    "        c_l_1 = c_1[non_zero_idxs]\n",
    "        h_l_1 = h_1[non_zero_idxs]\n",
    "        \n",
    "        c_l_2 = c_2[non_zero_idxs]\n",
    "        h_l_2 = h_2[non_zero_idxs]\n",
    "        \n",
    "        if np.all(batch_lens_aux == 0):\n",
    "            break\n",
    "    \n",
    "           \n",
    "        m, states_, _ = sess.run([merge, states, upd], feed_dict={x: batch_x, y: batch_y, init_state_c_1: c_l_1, init_state_h_1: h_l_1, init_state_c_2: c_l_2, init_state_h_2: h_l_2, seq_len: batch_lens_aux})\n",
    "        train_writer.add_summary(m, counter)\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "        c_1[non_zero_idxs] = states_[0].c\n",
    "        h_1[non_zero_idxs] = states_[0].h\n",
    "\n",
    "        c_2[non_zero_idxs] = states_[1].c\n",
    "        h_2[non_zero_idxs] = states_[1].h\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
